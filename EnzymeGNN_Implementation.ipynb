{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPd5CX0VnjoqbL/gZFMJda",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BtissamBalmane/FM_FunPDBe/blob/master/EnzymeGNN_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qz7SUSBFk8P",
        "outputId": "4a52f5f7-ff98-4a0c-84bb-0610cfe2aed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.14)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.0.2)\n",
            "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-geometric\n",
        "!pip install matplotlib numpy pandas scikit-learn\n",
        "!pip install biopython\n",
        "!pip install networkx\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to store data and results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directories for the project\n",
        "!mkdir -p /content/drive/MyDrive/EnzymeGNN/data\n",
        "!mkdir -p /content/drive/MyDrive/EnzymeGNN/models\n",
        "!mkdir -p /content/drive/MyDrive/EnzymeGNN/results\n",
        "!mkdir -p /content/drive/MyDrive/EnzymeGNN/visualizations\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGxFcQc9Pcnx",
        "outputId": "fabc62dc-8fa1-4c2c-a790-f5e76cd47106"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GeometricMessagePassing(nn.Module):\n",
        "    \"\"\"\n",
        "    Geometric message passing layer that explicitly encodes distances and angles.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, edge_dim=None):\n",
        "        super(GeometricMessagePassing, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        # Message function\n",
        "        self.message_fn = nn.Sequential(\n",
        "            nn.Linear(in_channels * 2 + (edge_dim or 0), out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(out_channels, out_channels)\n",
        "        )\n",
        "\n",
        "        # Update function\n",
        "        self.update_fn = nn.Sequential(\n",
        "            nn.Linear(in_channels + out_channels, out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(out_channels, out_channels)\n",
        "        )\n",
        "\n",
        "        # Distance encoding\n",
        "        self.distance_encoder = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 32)\n",
        "        )\n",
        "\n",
        "        # Angle encoding (if available)\n",
        "        self.angle_encoder = nn.Sequential(\n",
        "            nn.Linear(2, 16),  # sin and cos of angle\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 32)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None, angles=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the geometric message passing layer.\n",
        "        \"\"\"\n",
        "        # Extract source and target nodes\n",
        "        src, dst = edge_index\n",
        "\n",
        "        # Compute messages\n",
        "        src_features = x[src]\n",
        "        dst_features = x[dst]\n",
        "\n",
        "        # Concatenate source and destination features\n",
        "        if edge_attr is not None:\n",
        "            message_inputs = torch.cat([src_features, dst_features, edge_attr], dim=1)\n",
        "        else:\n",
        "            message_inputs = torch.cat([src_features, dst_features], dim=1)\n",
        "\n",
        "        # Apply message function\n",
        "        messages = self.message_fn(message_inputs)\n",
        "\n",
        "        # Aggregate messages (sum)\n",
        "        aggr_messages = torch.zeros_like(x[:, :self.out_channels])\n",
        "        aggr_messages.index_add_(0, dst, messages)\n",
        "\n",
        "        # Update node features\n",
        "        update_inputs = torch.cat([x, aggr_messages], dim=1)\n",
        "        updated_features = self.update_fn(update_inputs)\n",
        "\n",
        "        return updated_features\n",
        "\n",
        "\n",
        "class ActiveSiteAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention mechanism that focuses on functionally important regions (active sites).\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, hidden_dim=64):\n",
        "        super(ActiveSiteAttention, self).__init__()\n",
        "\n",
        "        # Active site score prediction\n",
        "        self.active_site_predictor = nn.Sequential(\n",
        "            nn.Linear(in_channels, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Attention network\n",
        "        self.attention_network = nn.Sequential(\n",
        "            nn.Linear(in_channels * 2 + 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        \"\"\"\n",
        "        Forward pass of the active site attention mechanism.\n",
        "        \"\"\"\n",
        "        # Predict active site scores\n",
        "        active_site_scores = self.active_site_predictor(x)\n",
        "\n",
        "        # Extract source and target nodes\n",
        "        src, dst = edge_index\n",
        "\n",
        "        # Compute attention inputs\n",
        "        src_features = x[src]\n",
        "        dst_features = x[dst]\n",
        "        src_scores = active_site_scores[src]\n",
        "        dst_scores = active_site_scores[dst]\n",
        "\n",
        "        # Concatenate features and scores\n",
        "        attention_inputs = torch.cat([src_features, dst_features, src_scores, dst_scores], dim=1)\n",
        "\n",
        "        # Compute attention weights\n",
        "        attention_logits = self.attention_network(attention_inputs)\n",
        "\n",
        "        # Apply softmax over destination nodes\n",
        "        attention_weights = []\n",
        "        for i in range(x.size(0)):\n",
        "            mask = (dst == i)\n",
        "            if mask.sum() > 0:\n",
        "                node_logits = attention_logits[mask]\n",
        "                node_weights = F.softmax(node_logits, dim=0)\n",
        "                attention_weights.append(node_weights)\n",
        "\n",
        "        return torch.cat(attention_weights, dim=0), active_site_scores\n",
        "\n",
        "\n",
        "class SequenceStructureFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional fusion module for integrating sequence and structure information.\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_dim, struct_dim, fusion_dim):\n",
        "        super(SequenceStructureFusion, self).__init__()\n",
        "\n",
        "        # Sequence to structure attention\n",
        "        self.seq_to_struct_attn = nn.MultiheadAttention(\n",
        "            embed_dim=seq_dim,\n",
        "            num_heads=4,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Structure to sequence attention\n",
        "        self.struct_to_seq_attn = nn.MultiheadAttention(\n",
        "            embed_dim=struct_dim,\n",
        "            num_heads=4,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Gating network\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(seq_dim + struct_dim, fusion_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        self.output_proj = nn.Linear(fusion_dim, fusion_dim)\n",
        "\n",
        "    def forward(self, seq_features, struct_features):\n",
        "        \"\"\"\n",
        "        Forward pass of the sequence-structure fusion module.\n",
        "        \"\"\"\n",
        "        # Sequence to structure attention\n",
        "        seq_to_struct, _ = self.seq_to_struct_attn(\n",
        "            query=seq_features,\n",
        "            key=struct_features,\n",
        "            value=struct_features\n",
        "        )\n",
        "\n",
        "        # Structure to sequence attention\n",
        "        struct_to_seq, _ = self.struct_to_seq_attn(\n",
        "            query=struct_features,\n",
        "            key=seq_features,\n",
        "            value=seq_features\n",
        "        )\n",
        "\n",
        "        # Global pooling\n",
        "        seq_to_struct_global = seq_to_struct.mean(dim=1)\n",
        "        struct_to_seq_global = struct_to_seq.mean(dim=1)\n",
        "\n",
        "        # Compute gating values\n",
        "        gate_input = torch.cat([seq_to_struct_global, struct_to_seq_global], dim=1)\n",
        "        gate_values = self.gate(gate_input)\n",
        "\n",
        "        # Weighted combination\n",
        "        fused_features = gate_values * seq_to_struct_global + (1 - gate_values) * struct_to_seq_global\n",
        "\n",
        "        # Final projection\n",
        "        output = self.output_proj(fused_features)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class HierarchicalPrediction(nn.Module):\n",
        "    \"\"\"\n",
        "    Hierarchical prediction framework for EC number classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_ec_level1=6, num_ec_level2=65,\n",
        "                 num_ec_level3=50, num_ec_level4=207):\n",
        "        super(HierarchicalPrediction, self).__init__()\n",
        "\n",
        "        # Level 1 prediction\n",
        "        self.level1_predictor = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_ec_level1)\n",
        "        )\n",
        "\n",
        "        # Level 2 prediction (conditioned on level 1)\n",
        "        self.level2_predictor = nn.Sequential(\n",
        "            nn.Linear(input_dim + num_ec_level1, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_ec_level2)\n",
        "        )\n",
        "\n",
        "        # Level 3 prediction (conditioned on levels 1 and 2)\n",
        "        self.level3_predictor = nn.Sequential(\n",
        "            nn.Linear(input_dim + num_ec_level1 + num_ec_level2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_ec_level3)\n",
        "        )\n",
        "\n",
        "        # Level 4 prediction (conditioned on levels 1, 2, and 3)\n",
        "        self.level4_predictor = nn.Sequential(\n",
        "            nn.Linear(input_dim + num_ec_level1 + num_ec_level2 + num_ec_level3, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_ec_level4)\n",
        "        )\n",
        "\n",
        "        # Thermostability prediction\n",
        "        self.thermo_predictor = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the hierarchical prediction framework.\n",
        "        \"\"\"\n",
        "        # Level 1 prediction\n",
        "        level1_logits = self.level1_predictor(x)\n",
        "        level1_probs = F.softmax(level1_logits, dim=1)\n",
        "\n",
        "        # Level 2 prediction (conditioned on level 1)\n",
        "        level2_input = torch.cat([x, level1_probs], dim=1)\n",
        "        level2_logits = self.level2_predictor(level2_input)\n",
        "        level2_probs = F.softmax(level2_logits, dim=1)\n",
        "\n",
        "        # Level 3 prediction (conditioned on levels 1 and 2)\n",
        "        level3_input = torch.cat([x, level1_probs, level2_probs], dim=1)\n",
        "        level3_logits = self.level3_predictor(level3_input)\n",
        "        level3_probs = F.softmax(level3_logits, dim=1)\n",
        "\n",
        "        # Level 4 prediction (conditioned on levels 1, 2, and 3)\n",
        "        level4_input = torch.cat([x, level1_probs, level2_probs, level3_probs], dim=1)\n",
        "        level4_logits = self.level4_predictor(level4_input)\n",
        "        level4_probs = F.softmax(level4_logits, dim=1)\n",
        "\n",
        "        # Thermostability prediction\n",
        "        thermo_pred = self.thermo_predictor(x)\n",
        "\n",
        "        return {\n",
        "            'level1': level1_logits,\n",
        "            'level2': level2_logits,\n",
        "            'level3': level3_logits,\n",
        "            'level4': level4_logits,\n",
        "            'thermo': thermo_pred\n",
        "        }\n",
        "\n",
        "class EnzymeGNN(nn.Module):\n",
        "    \"\"\"\n",
        "    EnzymeGNN: A multi-scale geometric graph neural network for enzyme function prediction.\n",
        "    \"\"\"\n",
        "    def __init__(self, atom_feature_dim=74, residue_feature_dim=54, hidden_dim=128,\n",
        "                 num_layers=6, fusion_dim=256):\n",
        "        super(EnzymeGNN, self).__init__()\n",
        "\n",
        "        # Atom-level embedding\n",
        "        self.atom_embedding = nn.Linear(atom_feature_dim, hidden_dim)\n",
        "\n",
        "        # Residue-level embedding\n",
        "        self.residue_embedding = nn.Linear(residue_feature_dim, hidden_dim)\n",
        "\n",
        "        # Atom-level message passing layers\n",
        "        self.atom_layers = nn.ModuleList([\n",
        "            GeometricMessagePassing(\n",
        "                in_channels=hidden_dim,\n",
        "                out_channels=hidden_dim,\n",
        "                edge_dim=32  # Distance + angle features\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Residue-level message passing layers\n",
        "        self.residue_layers = nn.ModuleList([\n",
        "            GeometricMessagePassing(\n",
        "                in_channels=hidden_dim,\n",
        "                out_channels=hidden_dim,\n",
        "                edge_dim=32  # Distance + angle features\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Active site attention mechanism\n",
        "        self.active_site_attention = ActiveSiteAttention(\n",
        "            in_channels=hidden_dim\n",
        "        )\n",
        "\n",
        "        # Atom to residue pooling\n",
        "        self.atom_to_residue_pool = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Residue to atom unpooling\n",
        "        self.residue_to_atom_unpool = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Sequence-structure fusion\n",
        "        self.seq_struct_fusion = SequenceStructureFusion(\n",
        "            seq_dim=hidden_dim,\n",
        "            struct_dim=hidden_dim,\n",
        "            fusion_dim=fusion_dim\n",
        "        )\n",
        "\n",
        "        # Hierarchical prediction\n",
        "        self.hierarchical_prediction = HierarchicalPrediction(\n",
        "            input_dim=fusion_dim\n",
        "        )\n",
        "\n",
        "        # Readout function (global pooling)\n",
        "        self.readout = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Forward pass of EnzymeGNN.\n",
        "        \"\"\"\n",
        "        # Extract input data\n",
        "        atom_features = data['atom_features']\n",
        "        atom_edge_index = data['atom_edge_index']\n",
        "        atom_edge_attr = data.get('atom_edge_attr')\n",
        "\n",
        "        residue_features = data['residue_features']\n",
        "        residue_edge_index = data['residue_edge_index']\n",
        "        residue_edge_attr = data.get('residue_edge_attr')\n",
        "\n",
        "        atom_to_residue_mapping = data['atom_to_residue_mapping']\n",
        "\n",
        "        # Handle sequence_features which might be a list\n",
        "        sequence_features = data['sequence_features']\n",
        "        if isinstance(sequence_features, list):\n",
        "            # Convert list to tensor if needed\n",
        "            if len(sequence_features) > 0 and isinstance(sequence_features[0], torch.Tensor):\n",
        "                sequence_features = torch.stack(sequence_features)\n",
        "            else:\n",
        "                # Create a dummy tensor if sequence_features can't be converted\n",
        "                sequence_features = torch.zeros((1, residue_features.size(0), 20), device=residue_features.device)\n",
        "\n",
        "        # Debug prints\n",
        "        print(\"Inside model - atom_features shape:\", atom_features.shape)\n",
        "        print(\"Inside model - residue_features shape:\", residue_features.shape)\n",
        "        print(\"Inside model - sequence_features shape:\",\n",
        "              sequence_features.shape if isinstance(sequence_features, torch.Tensor) else \"not a tensor\")\n",
        "\n",
        "        # Initial embeddings\n",
        "        atom_h = self.atom_embedding(atom_features)\n",
        "        residue_h = self.residue_embedding(residue_features)\n",
        "\n",
        "        # Atom-level message passing\n",
        "        for layer in self.atom_layers:\n",
        "            atom_h = layer(atom_h, atom_edge_index, atom_edge_attr)\n",
        "\n",
        "        # Apply active site attention at atom level\n",
        "        atom_attention_weights, atom_active_site_scores = self.active_site_attention(\n",
        "            atom_h, atom_edge_index\n",
        "        )\n",
        "\n",
        "        # Pool atom features to residue level\n",
        "        pooled_atom_h = torch.zeros_like(residue_h)\n",
        "        for atom_idx, residue_idx in enumerate(atom_to_residue_mapping):\n",
        "            pooled_atom_h[residue_idx] += atom_h[atom_idx]\n",
        "\n",
        "        # Combine pooled atom features with residue features\n",
        "        residue_h = residue_h + self.atom_to_residue_pool(pooled_atom_h)\n",
        "\n",
        "        # Residue-level message passing\n",
        "        for layer in self.residue_layers:\n",
        "            residue_h = layer(residue_h, residue_edge_index, residue_edge_attr)\n",
        "\n",
        "        # Apply active site attention at residue level\n",
        "        residue_attention_weights, residue_active_site_scores = self.active_site_attention(\n",
        "            residue_h, residue_edge_index\n",
        "        )\n",
        "\n",
        "        # Unpool residue features to atom level\n",
        "        unpooled_residue_h = self.residue_to_atom_unpool(residue_h[atom_to_residue_mapping])\n",
        "\n",
        "        # Combine unpooled residue features with atom features\n",
        "        atom_h = atom_h + unpooled_residue_h\n",
        "\n",
        "        # Global pooling for structure representation - simplify for debugging\n",
        "        structure_features = torch.mean(residue_h, dim=0, keepdim=True).unsqueeze(0)  # [1, 1, hidden_dim]\n",
        "\n",
        "        # For debugging, use a simplified approach\n",
        "        # Skip the sequence-structure fusion and use only structure features\n",
        "        fused_features = structure_features.squeeze(0)  # [1, hidden_dim]\n",
        "\n",
        "        # Debug prints for fusion\n",
        "        print(\"Structure features shape:\", structure_features.shape)\n",
        "        print(\"Fused features shape:\", fused_features.shape)\n",
        "\n",
        "        # Hierarchical prediction\n",
        "        predictions = self.hierarchical_prediction(fused_features)\n",
        "\n",
        "        # Add attention weights and active site scores to predictions\n",
        "        predictions['atom_attention'] = atom_attention_weights\n",
        "        predictions['residue_attention'] = residue_attention_weights\n",
        "        predictions['atom_active_sites'] = atom_active_site_scores\n",
        "        predictions['residue_active_sites'] = residue_active_site_scores\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "\n",
        "# Loss function for multi-task learning\n",
        "class EnzymeGNNLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-task loss function for EnzymeGNN.\n",
        "    \"\"\"\n",
        "    def __init__(self, ec_weights=[1.0, 1.0, 1.0, 1.0], thermo_weight=1.0, active_site_weight=0.5):\n",
        "        super(EnzymeGNNLoss, self).__init__()\n",
        "        self.ec_weights = ec_weights\n",
        "        self.thermo_weight = thermo_weight\n",
        "        self.active_site_weight = active_site_weight\n",
        "\n",
        "        # Loss functions\n",
        "        self.ec_loss_fn = nn.CrossEntropyLoss()\n",
        "        self.thermo_loss_fn = nn.MSELoss()\n",
        "        self.active_site_loss_fn = nn.BCELoss()\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        Compute the multi-task loss.\n",
        "        \"\"\"\n",
        "        # EC number classification losses\n",
        "        ec_level1_loss = self.ec_loss_fn(predictions['level1'], targets['level1'])\n",
        "        ec_level2_loss = self.ec_loss_fn(predictions['level2'], targets['level2'])\n",
        "        ec_level3_loss = self.ec_loss_fn(predictions['level3'], targets['level3'])\n",
        "        ec_level4_loss = self.ec_loss_fn(predictions['level4'], targets['level4'])\n",
        "\n",
        "        # Thermostability regression loss\n",
        "        thermo_loss = self.thermo_loss_fn(predictions['thermo'], targets['thermo'])\n",
        "\n",
        "        # Active site prediction loss (if ground truth is available)\n",
        "        active_site_loss = 0.0\n",
        "        if 'active_sites' in targets:\n",
        "            atom_active_site_loss = self.active_site_loss_fn(\n",
        "                predictions['atom_active_sites'],\n",
        "                targets['active_sites'][targets['atom_to_residue_mapping']]\n",
        "            )\n",
        "            residue_active_site_loss = self.active_site_loss_fn(\n",
        "                predictions['residue_active_sites'],\n",
        "                targets['active_sites']\n",
        "            )\n",
        "            active_site_loss = atom_active_site_loss + residue_active_site_loss\n",
        "\n",
        "        # Weighted sum of losses\n",
        "        total_loss = (\n",
        "            self.ec_weights[0] * ec_level1_loss +\n",
        "            self.ec_weights[1] * ec_level2_loss +\n",
        "            self.ec_weights[2] * ec_level3_loss +\n",
        "            self.ec_weights[3] * ec_level4_loss +\n",
        "            self.thermo_weight * thermo_loss +\n",
        "            self.active_site_weight * active_site_loss\n",
        "        )\n",
        "\n",
        "        # Return total loss and individual components\n",
        "        return {\n",
        "            'total': total_loss,\n",
        "            'ec_level1': ec_level1_loss,\n",
        "            'ec_level2': ec_level2_loss,\n",
        "            'ec_level3': ec_level3_loss,\n",
        "            'ec_level4': ec_level4_loss,\n",
        "            'thermo': thermo_loss,\n",
        "            'active_site': active_site_loss\n",
        "        }\n"
      ],
      "metadata": {
        "id": "O-Jr3l2cP9Vu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from Bio.PDB import PDBParser, Selection\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "class ProteinGraphDataset:\n",
        "    \"\"\"\n",
        "    Dataset for processing protein structures into multi-scale graphs.\n",
        "    \"\"\"\n",
        "    def __init__(self, pdb_dir, csv_file=None, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "        \"\"\"\n",
        "        self.pdb_dir = pdb_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load metadata if available\n",
        "        if csv_file is not None and os.path.exists(csv_file):\n",
        "            self.metadata = pd.read_csv(csv_file)\n",
        "            self.pdb_files = [f\"{id}.pdb\" for id in self.metadata['protein_id']]\n",
        "        else:\n",
        "            self.metadata = None\n",
        "            self.pdb_files = [f for f in os.listdir(pdb_dir) if f.endswith('.pdb')]\n",
        "\n",
        "        # PDB parser\n",
        "        self.parser = PDBParser(QUIET=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pdb_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get PDB file path\n",
        "        pdb_file = os.path.join(self.pdb_dir, self.pdb_files[idx])\n",
        "\n",
        "        # Get metadata if available\n",
        "        if self.metadata is not None:\n",
        "            protein_id = self.metadata.iloc[idx]['protein_id']\n",
        "            ec_numbers = self.metadata.iloc[idx]['ec_numbers'].split('.')\n",
        "            thermostability = self.metadata.iloc[idx]['thermostability']\n",
        "            sequence = self.metadata.iloc[idx]['sequence']\n",
        "        else:\n",
        "            protein_id = self.pdb_files[idx].split('.')[0]\n",
        "            ec_numbers = None\n",
        "            thermostability = None\n",
        "            sequence = None\n",
        "\n",
        "        # Process PDB file\n",
        "        data = self.process_pdb(pdb_file, protein_id, sequence)\n",
        "\n",
        "        # Add targets if available\n",
        "        if ec_numbers is not None:\n",
        "            data['targets'] = {\n",
        "                'level1': torch.tensor(int(ec_numbers[0])),\n",
        "                'level2': torch.tensor(int(ec_numbers[1])),\n",
        "                'level3': torch.tensor(int(ec_numbers[2])),\n",
        "                'level4': torch.tensor(int(ec_numbers[3])),\n",
        "                'thermo': torch.tensor(thermostability, dtype=torch.float)\n",
        "            }\n",
        "\n",
        "        # Apply transform if available\n",
        "        if self.transform is not None:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def process_pdb(self, pdb_file, protein_id, sequence=None):\n",
        "        \"\"\"\n",
        "        Process a PDB file into a multi-scale graph.\n",
        "        \"\"\"\n",
        "        # Parse PDB file\n",
        "        structure = self.parser.get_structure(protein_id, pdb_file)\n",
        "        model = structure[0]  # First model\n",
        "\n",
        "        # Extract atoms and residues\n",
        "        atoms = list(model.get_atoms())\n",
        "        residues = list(model.get_residues())\n",
        "\n",
        "        # Compute atom features\n",
        "        atom_features = []\n",
        "        atom_positions = []\n",
        "        atom_to_residue_mapping = []\n",
        "\n",
        "        for atom_idx, atom in enumerate(atoms):\n",
        "            # Get atom features\n",
        "            atom_feat = self.get_atom_features(atom)\n",
        "            atom_features.append(atom_feat)\n",
        "\n",
        "            # Get atom position\n",
        "            atom_pos = atom.get_coord()\n",
        "            atom_positions.append(atom_pos)\n",
        "\n",
        "            # Map atom to residue\n",
        "            residue_idx = residues.index(atom.get_parent())\n",
        "            atom_to_residue_mapping.append(residue_idx)\n",
        "\n",
        "        # Convert to tensors\n",
        "        atom_features = torch.tensor(np.array(atom_features), dtype=torch.float)\n",
        "        atom_positions = torch.tensor(np.array(atom_positions), dtype=torch.float)\n",
        "        atom_to_residue_mapping = torch.tensor(atom_to_residue_mapping, dtype=torch.long)\n",
        "\n",
        "        # Compute residue features\n",
        "        residue_features = []\n",
        "        residue_positions = []\n",
        "\n",
        "        for residue_idx, residue in enumerate(residues):\n",
        "            # Get residue features\n",
        "            residue_feat = self.get_residue_features(residue)\n",
        "            residue_features.append(residue_feat)\n",
        "\n",
        "            # Get residue position (CA atom)\n",
        "            ca_atom = residue['CA'] if 'CA' in residue else next(residue.get_atoms())\n",
        "            residue_pos = ca_atom.get_coord()\n",
        "            residue_positions.append(residue_pos)\n",
        "\n",
        "        # Convert to tensors\n",
        "        residue_features = torch.tensor(np.array(residue_features), dtype=torch.float)\n",
        "        residue_positions = torch.tensor(np.array(residue_positions), dtype=torch.float)\n",
        "\n",
        "        # Compute atom-level graph connectivity\n",
        "        atom_edge_index, atom_edge_attr = self.compute_graph_connectivity(\n",
        "            atom_positions, cutoff=4.5\n",
        "        )\n",
        "\n",
        "        # Compute residue-level graph connectivity\n",
        "        residue_edge_index, residue_edge_attr = self.compute_graph_connectivity(\n",
        "            residue_positions, cutoff=10.0\n",
        "        )\n",
        "\n",
        "        # Get sequence features if available\n",
        "        if sequence is not None:\n",
        "            sequence_features = self.get_sequence_features(sequence)\n",
        "        else:\n",
        "            # Extract sequence from residues\n",
        "            sequence = ''.join([self.get_residue_code(r) for r in residues])\n",
        "            sequence_features = self.get_sequence_features(sequence)\n",
        "\n",
        "        # Create data dictionary\n",
        "        data = {\n",
        "            'protein_id': protein_id,\n",
        "            'atom_features': atom_features,\n",
        "            'atom_positions': atom_positions,\n",
        "            'atom_edge_index': atom_edge_index,\n",
        "            'atom_edge_attr': atom_edge_attr,\n",
        "            'residue_features': residue_features,\n",
        "            'residue_positions': residue_positions,\n",
        "            'residue_edge_index': residue_edge_index,\n",
        "            'residue_edge_attr': residue_edge_attr,\n",
        "            'atom_to_residue_mapping': atom_to_residue_mapping,\n",
        "            'sequence': sequence,\n",
        "            'sequence_features': sequence_features\n",
        "        }\n",
        "\n",
        "        return data\n",
        "\n",
        "    def get_atom_features(self, atom):\n",
        "        \"\"\"\n",
        "        Extract features for an atom.\n",
        "        \"\"\"\n",
        "        # Atom type (one-hot encoding)\n",
        "        atom_types = ['C', 'N', 'O', 'S', 'P', 'H', 'F', 'Cl', 'Br', 'I']\n",
        "        atom_type_onehot = [1 if atom.element == t else 0 for t in atom_types]\n",
        "\n",
        "        # Hybridization (estimated from atom name and residue)\n",
        "        hybridization = self.estimate_hybridization(atom)\n",
        "\n",
        "        # Partial charge (estimated)\n",
        "        partial_charge = self.estimate_partial_charge(atom)\n",
        "\n",
        "        # Is in aromatic ring\n",
        "        is_aromatic = self.is_in_aromatic_ring(atom)\n",
        "\n",
        "        # Is backbone atom\n",
        "        is_backbone = atom.name in ['N', 'CA', 'C', 'O']\n",
        "\n",
        "        # Combine features\n",
        "        features = atom_type_onehot + [hybridization, partial_charge, is_aromatic, is_backbone]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def estimate_hybridization(self, atom):\n",
        "        \"\"\"Estimate atom hybridization (simplified)\"\"\"\n",
        "        if atom.element == 'C':\n",
        "            # Count bonds (simplified)\n",
        "            parent = atom.get_parent()\n",
        "            neighbors = [a for a in parent.get_atoms() if a != atom]\n",
        "            num_neighbors = len([n for n in neighbors if self.is_bonded(atom, n)])\n",
        "\n",
        "            if num_neighbors == 4:\n",
        "                return 0  # sp3\n",
        "            elif num_neighbors == 3:\n",
        "                return 1  # sp2\n",
        "            elif num_neighbors == 2:\n",
        "                return 2  # sp\n",
        "            else:\n",
        "                return 0  # default to sp3\n",
        "        elif atom.element == 'N':\n",
        "            return 1  # default to sp2 for nitrogen\n",
        "        elif atom.element == 'O':\n",
        "            return 0  # default to sp3 for oxygen\n",
        "        else:\n",
        "            return 0  # default\n",
        "\n",
        "    def estimate_partial_charge(self, atom):\n",
        "        \"\"\"Estimate atom partial charge (simplified)\"\"\"\n",
        "        if atom.element == 'O':\n",
        "            return -0.5\n",
        "        elif atom.element == 'N':\n",
        "            return -0.3\n",
        "        elif atom.element == 'S':\n",
        "            return -0.2\n",
        "        elif atom.element == 'C':\n",
        "            if atom.name == 'C':  # Carbonyl carbon\n",
        "                return 0.5\n",
        "            else:\n",
        "                return 0.1\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    def is_in_aromatic_ring(self, atom):\n",
        "        \"\"\"Check if atom is in an aromatic ring (simplified)\"\"\"\n",
        "        residue = atom.get_parent()\n",
        "        residue_name = residue.get_resname()\n",
        "\n",
        "        # Check if in aromatic residue and is part of ring\n",
        "        if residue_name in ['PHE', 'TYR', 'TRP', 'HIS']:\n",
        "            if atom.name in ['CG', 'CD1', 'CD2', 'CE1', 'CE2', 'CZ', 'CZ2', 'CZ3', 'CH2']:\n",
        "                return 1\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def is_bonded(self, atom1, atom2):\n",
        "        \"\"\"Check if two atoms are bonded (simplified)\"\"\"\n",
        "        # Distance-based criterion\n",
        "        distance = np.linalg.norm(atom1.get_coord() - atom2.get_coord())\n",
        "        return distance < 2.0  # Typical bond length threshold\n",
        "\n",
        "    def get_residue_features(self, residue):\n",
        "        \"\"\"\n",
        "        Extract features for a residue.\n",
        "        \"\"\"\n",
        "        # Residue type (one-hot encoding)\n",
        "        residue_types = [\n",
        "            'ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS', 'ILE',\n",
        "            'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL'\n",
        "        ]\n",
        "        residue_type_onehot = [1 if residue.get_resname() == t else 0 for t in residue_types]\n",
        "\n",
        "        # Physicochemical properties\n",
        "        properties = self.get_residue_properties(residue)\n",
        "\n",
        "        # Secondary structure (simplified)\n",
        "        ss_features = self.get_secondary_structure_features(residue)\n",
        "\n",
        "        # Combine features\n",
        "        features = residue_type_onehot + properties + ss_features\n",
        "\n",
        "        return features\n",
        "\n",
        "    def get_residue_properties(self, residue):\n",
        "        \"\"\"Get physicochemical properties of residue\"\"\"\n",
        "        # Properties: hydrophobicity, charge, polarity, size\n",
        "        residue_name = residue.get_resname()\n",
        "\n",
        "        # Hydrophobicity scale (Kyte-Doolittle)\n",
        "        hydrophobicity = {\n",
        "            'ALA': 1.8, 'ARG': -4.5, 'ASN': -3.5, 'ASP': -3.5, 'CYS': 2.5,\n",
        "            'GLN': -3.5, 'GLU': -3.5, 'GLY': -0.4, 'HIS': -3.2, 'ILE': 4.5,\n",
        "            'LEU': 3.8, 'LYS': -3.9, 'MET': 1.9, 'PHE': 2.8, 'PRO': -1.6,\n",
        "            'SER': -0.8, 'THR': -0.7, 'TRP': -0.9, 'TYR': -1.3, 'VAL': 4.2\n",
        "        }\n",
        "\n",
        "        # Charge at pH 7\n",
        "        charge = {\n",
        "            'ARG': 1, 'LYS': 1, 'ASP': -1, 'GLU': -1, 'HIS': 0.1,\n",
        "            'ALA': 0, 'ASN': 0, 'CYS': 0, 'GLN': 0, 'GLY': 0,\n",
        "            'ILE': 0, 'LEU': 0, 'MET': 0, 'PHE': 0, 'PRO': 0,\n",
        "            'SER': 0, 'THR': 0, 'TRP': 0, 'TYR': 0, 'VAL': 0\n",
        "        }\n",
        "\n",
        "        # Polarity\n",
        "        polarity = {\n",
        "            'ARG': 1, 'ASN': 1, 'ASP': 1, 'GLN': 1, 'GLU': 1,\n",
        "            'HIS': 1, 'LYS': 1, 'SER': 1, 'THR': 1, 'TYR': 1,\n",
        "            'ALA': 0, 'CYS': 0, 'GLY': 0, 'ILE': 0, 'LEU': 0,\n",
        "            'MET': 0, 'PHE': 0, 'PRO': 0, 'TRP': 0, 'VAL': 0\n",
        "        }\n",
        "\n",
        "        # Size (volume in Å³)\n",
        "        size = {\n",
        "            'ALA': 88.6, 'ARG': 173.4, 'ASN': 114.1, 'ASP': 111.1, 'CYS': 108.5,\n",
        "            'GLN': 143.8, 'GLU': 138.4, 'GLY': 60.1, 'HIS': 153.2, 'ILE': 166.7,\n",
        "            'LEU': 166.7, 'LYS': 168.6, 'MET': 162.9, 'PHE': 189.9, 'PRO': 112.7,\n",
        "            'SER': 89.0, 'THR': 116.1, 'TRP': 227.8, 'TYR': 193.6, 'VAL': 140.0\n",
        "        }\n",
        "\n",
        "        # Normalize values\n",
        "        hydrophobicity_norm = (hydrophobicity.get(residue_name, 0) + 4.5) / 9.0\n",
        "        size_norm = size.get(residue_name, 0) / 227.8\n",
        "\n",
        "        return [\n",
        "            hydrophobicity_norm,\n",
        "            charge.get(residue_name, 0),\n",
        "            polarity.get(residue_name, 0),\n",
        "            size_norm\n",
        "        ]\n",
        "\n",
        "    def get_secondary_structure_features(self, residue):\n",
        "        \"\"\"Get secondary structure features (simplified)\"\"\"\n",
        "        # This is a simplified version; ideally, use DSSP\n",
        "        # Here we use a heuristic based on residue type\n",
        "\n",
        "        residue_name = residue.get_resname()\n",
        "\n",
        "        # Propensities for different secondary structures\n",
        "        helix_propensity = {\n",
        "            'ALA': 1.0, 'ARG': 0.7, 'ASN': 0.5, 'ASP': 0.5, 'CYS': 0.5,\n",
        "            'GLN': 0.8, 'GLU': 0.8, 'GLY': 0.0, 'HIS': 0.5, 'ILE': 0.8,\n",
        "            'LEU': 0.9, 'LYS': 0.7, 'MET': 0.8, 'PHE': 0.6, 'PRO': 0.0,\n",
        "            'SER': 0.4, 'THR': 0.4, 'TRP': 0.6, 'TYR': 0.6, 'VAL': 0.7\n",
        "        }\n",
        "\n",
        "        sheet_propensity = {\n",
        "            'ALA': 0.5, 'ARG': 0.4, 'ASN': 0.3, 'ASP': 0.3, 'CYS': 0.6,\n",
        "            'GLN': 0.4, 'GLU': 0.4, 'GLY': 0.0, 'HIS': 0.5, 'ILE': 1.0,\n",
        "            'LEU': 0.7, 'LYS': 0.4, 'MET': 0.6, 'PHE': 0.8, 'PRO': 0.0,\n",
        "            'SER': 0.3, 'THR': 0.5, 'TRP': 0.8, 'TYR': 0.8, 'VAL': 1.0\n",
        "        }\n",
        "\n",
        "        turn_propensity = {\n",
        "            'ALA': 0.3, 'ARG': 0.6, 'ASN': 0.8, 'ASP': 0.8, 'CYS': 0.4,\n",
        "            'GLN': 0.6, 'GLU': 0.6, 'GLY': 1.0, 'HIS': 0.5, 'ILE': 0.2,\n",
        "            'LEU': 0.3, 'LYS': 0.6, 'MET': 0.3, 'PHE': 0.3, 'PRO': 1.0,\n",
        "            'SER': 0.7, 'THR': 0.6, 'TRP': 0.3, 'TYR': 0.3, 'VAL': 0.2\n",
        "        }\n",
        "\n",
        "        return [\n",
        "            helix_propensity.get(residue_name, 0.5),\n",
        "            sheet_propensity.get(residue_name, 0.5),\n",
        "            turn_propensity.get(residue_name, 0.5)\n",
        "        ]\n",
        "\n",
        "    def get_residue_code(self, residue):\n",
        "        \"\"\"Convert residue to one-letter code\"\"\"\n",
        "        residue_name = residue.get_resname()\n",
        "        code_map = {\n",
        "            'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',\n",
        "            'GLN': 'Q', 'GLU': 'E', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
        "            'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',\n",
        "            'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
        "        }\n",
        "        return code_map.get(residue_name, 'X')\n",
        "\n",
        "    def get_sequence_features(self, sequence):\n",
        "        \"\"\"\n",
        "        Extract features from protein sequence.\n",
        "        \"\"\"\n",
        "        # One-hot encoding of amino acids\n",
        "        aa_types = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "        # Initialize features\n",
        "        seq_features = []\n",
        "\n",
        "        for aa in sequence:\n",
        "            # One-hot encoding\n",
        "            aa_onehot = [1 if aa == t else 0 for t in aa_types]\n",
        "            seq_features.append(aa_onehot)\n",
        "\n",
        "        # Convert to tensor\n",
        "        seq_features = torch.tensor(np.array(seq_features), dtype=torch.float)\n",
        "\n",
        "        return seq_features\n",
        "\n",
        "    def compute_graph_connectivity(self, positions, cutoff):\n",
        "        \"\"\"\n",
        "        Compute graph connectivity based on distance cutoff.\n",
        "        \"\"\"\n",
        "        # Convert to numpy for efficient computation\n",
        "        positions_np = positions.numpy()\n",
        "\n",
        "        # Compute pairwise distances\n",
        "        num_nodes = positions_np.shape[0]\n",
        "        edges = []\n",
        "        edge_features = []\n",
        "\n",
        "        for i in range(num_nodes):\n",
        "            for j in range(num_nodes):\n",
        "                if i != j:\n",
        "                    # Compute distance\n",
        "                    distance = np.linalg.norm(positions_np[i] - positions_np[j])\n",
        "\n",
        "                    # Add edge if within cutoff\n",
        "                    if distance < cutoff:\n",
        "                        edges.append([i, j])\n",
        "\n",
        "                        # Compute edge features\n",
        "                        direction = positions_np[j] - positions_np[i]\n",
        "                        unit_vec = direction / distance\n",
        "\n",
        "                        # Edge features: distance and unit vector\n",
        "                        edge_feat = [distance] + unit_vec.tolist()\n",
        "                        edge_features.append(edge_feat)\n",
        "\n",
        "        # Convert to tensors\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
        "        edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
        "\n",
        "        return edge_index, edge_attr\n",
        "\n",
        "\n",
        "# Example usage\n",
        "def create_dataloader(pdb_dir, csv_file=None, batch_size=32, shuffle=True):\n",
        "    \"\"\"\n",
        "    Create a dataloader for protein graphs.\n",
        "    \"\"\"\n",
        "    # Create dataset\n",
        "    dataset = ProteinGraphDataset(pdb_dir, csv_file)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_protein_graphs\n",
        "    )\n",
        "\n",
        "    return dataloader\n",
        "def collate_protein_graphs(batch):\n",
        "    \"\"\"\n",
        "    Collate function for protein graphs.\n",
        "    \"\"\"\n",
        "    # Initialize batched data\n",
        "    batched_data = {\n",
        "        'protein_ids': [],\n",
        "        'atom_features': [],\n",
        "        'atom_positions': [],\n",
        "        'atom_edge_index': [],\n",
        "        'atom_edge_attr': [],\n",
        "        'residue_features': [],\n",
        "        'residue_positions': [],\n",
        "        'residue_edge_index': [],\n",
        "        'residue_edge_attr': [],\n",
        "        'atom_to_residue_mapping': [],\n",
        "        'sequence_features': [],\n",
        "        'targets': {\n",
        "            'level1': [],\n",
        "            'level2': [],\n",
        "            'level3': [],\n",
        "            'level4': [],\n",
        "            'thermo': []\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Cumulative number of atoms and residues\n",
        "    cum_atoms = 0\n",
        "    cum_residues = 0\n",
        "\n",
        "    # Process each sample\n",
        "    for data in batch:\n",
        "        # Add protein ID\n",
        "        batched_data['protein_ids'].append(data['protein_id'])\n",
        "\n",
        "        # Add atom features and positions\n",
        "        batched_data['atom_features'].append(data['atom_features'])\n",
        "        batched_data['atom_positions'].append(data['atom_positions'])\n",
        "\n",
        "        # Add residue features and positions\n",
        "        batched_data['residue_features'].append(data['residue_features'])\n",
        "        batched_data['residue_positions'].append(data['residue_positions'])\n",
        "\n",
        "        # Add sequence features - store as is, will handle in model\n",
        "        if 'sequence_features' in data:\n",
        "            batched_data['sequence_features'].append(data['sequence_features'])\n",
        "\n",
        "        # Add atom edge index and attributes with offset\n",
        "        if cum_atoms > 0:\n",
        "            atom_edge_index = data['atom_edge_index'] + cum_atoms\n",
        "        else:\n",
        "            atom_edge_index = data['atom_edge_index']\n",
        "        batched_data['atom_edge_index'].append(atom_edge_index)\n",
        "        batched_data['atom_edge_attr'].append(data['atom_edge_attr'])\n",
        "\n",
        "        # Add residue edge index and attributes with offset\n",
        "        if cum_residues > 0:\n",
        "            residue_edge_index = data['residue_edge_index'] + cum_residues\n",
        "        else:\n",
        "            residue_edge_index = data['residue_edge_index']\n",
        "        batched_data['residue_edge_index'].append(residue_edge_index)\n",
        "        batched_data['residue_edge_attr'].append(data['residue_edge_attr'])\n",
        "\n",
        "        # Add atom to residue mapping with offset\n",
        "        atom_to_residue_mapping = data['atom_to_residue_mapping'] + cum_residues\n",
        "        batched_data['atom_to_residue_mapping'].append(atom_to_residue_mapping)\n",
        "\n",
        "        # Add targets if available\n",
        "        if 'targets' in data:\n",
        "            for level in ['level1', 'level2', 'level3', 'level4', 'thermo']:\n",
        "                batched_data['targets'][level].append(data['targets'][level])\n",
        "\n",
        "        # Update cumulative counts\n",
        "        cum_atoms += data['atom_features'].shape[0]\n",
        "        cum_residues += data['residue_features'].shape[0]\n",
        "\n",
        "    # Concatenate tensors\n",
        "    batched_data['atom_features'] = torch.cat(batched_data['atom_features'], dim=0)\n",
        "    batched_data['atom_positions'] = torch.cat(batched_data['atom_positions'], dim=0)\n",
        "    batched_data['atom_edge_index'] = torch.cat(batched_data['atom_edge_index'], dim=1)\n",
        "    batched_data['atom_edge_attr'] = torch.cat(batched_data['atom_edge_attr'], dim=0)\n",
        "\n",
        "    batched_data['residue_features'] = torch.cat(batched_data['residue_features'], dim=0)\n",
        "    batched_data['residue_positions'] = torch.cat(batched_data['residue_positions'], dim=0)\n",
        "    batched_data['residue_edge_index'] = torch.cat(batched_data['residue_edge_index'], dim=1)\n",
        "    batched_data['residue_edge_attr'] = torch.cat(batched_data['residue_edge_attr'], dim=0)\n",
        "\n",
        "    batched_data['atom_to_residue_mapping'] = torch.cat(batched_data['atom_to_residue_mapping'], dim=0)\n",
        "\n",
        "    # Keep sequence_features as a list - will handle in model\n",
        "\n",
        "    # Stack targets if available\n",
        "    if batched_data['targets']['level1']:\n",
        "        for level in ['level1', 'level2', 'level3', 'level4']:\n",
        "            batched_data['targets'][level] = torch.stack(batched_data['targets'][level])\n",
        "        batched_data['targets']['thermo'] = torch.stack(batched_data['targets']['thermo'])\n",
        "    else:\n",
        "        batched_data.pop('targets')\n",
        "\n",
        "    return batched_data\n",
        "\n"
      ],
      "metadata": {
        "id": "upay2PKcSw6X"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
        "                num_epochs=50, device='cpu', checkpoint_dir='/content/drive/MyDrive/EnzymeGNN/models'):\n",
        "    \"\"\"\n",
        "    Train the EnzymeGNN model.\n",
        "    \"\"\"\n",
        "    # Create checkpoint directory if it doesn't exist\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'ec_level1_acc': [],\n",
        "        'ec_level4_acc': [],\n",
        "        'thermo_mae': []\n",
        "    }\n",
        "\n",
        "    # Best validation loss\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            # Move data to device\n",
        "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "            targets = {k: v.to(device) for k, v in batch['targets'].items()}\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(batch)\n",
        "\n",
        "            # Compute loss\n",
        "            loss_dict = criterion(predictions, targets)\n",
        "            loss = loss_dict['total']\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update training loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Print progress\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Average training loss\n",
        "        train_loss /= len(train_loader)\n",
        "        history['train_loss'].append(train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        ec_level1_preds = []\n",
        "        ec_level1_targets = []\n",
        "        ec_level4_preds = []\n",
        "        ec_level4_targets = []\n",
        "        thermo_preds = []\n",
        "        thermo_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                # Move data to device\n",
        "                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "                targets = {k: v.to(device) for k, v in batch['targets'].items()}\n",
        "\n",
        "                # Forward pass\n",
        "                predictions = model(batch)\n",
        "\n",
        "                # Compute loss\n",
        "                loss_dict = criterion(predictions, targets)\n",
        "                val_loss += loss_dict['total'].item()\n",
        "\n",
        "                # Collect predictions and targets for metrics\n",
        "                ec_level1_preds.append(predictions['level1'].argmax(dim=1).cpu().numpy())\n",
        "                ec_level1_targets.append(targets['level1'].cpu().numpy())\n",
        "\n",
        "                ec_level4_preds.append(predictions['level4'].argmax(dim=1).cpu().numpy())\n",
        "                ec_level4_targets.append(targets['level4'].cpu().numpy())\n",
        "\n",
        "                thermo_preds.append(predictions['thermo'].cpu().numpy())\n",
        "                thermo_targets.append(targets['thermo'].cpu().numpy())\n",
        "\n",
        "        # Average validation loss\n",
        "        val_loss /= len(val_loader)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        # Compute metrics\n",
        "        ec_level1_preds = np.concatenate(ec_level1_preds)\n",
        "        ec_level1_targets = np.concatenate(ec_level1_targets)\n",
        "        ec_level1_acc = accuracy_score(ec_level1_targets, ec_level1_preds)\n",
        "        history['ec_level1_acc'].append(ec_level1_acc)\n",
        "\n",
        "        ec_level4_preds = np.concatenate(ec_level4_preds)\n",
        "        ec_level4_targets = np.concatenate(ec_level4_targets)\n",
        "        ec_level4_acc = accuracy_score(ec_level4_targets, ec_level4_preds)\n",
        "        history['ec_level4_acc'].append(ec_level4_acc)\n",
        "\n",
        "        thermo_preds = np.concatenate(thermo_preds)\n",
        "        thermo_targets = np.concatenate(thermo_targets)\n",
        "        thermo_mae = mean_absolute_error(thermo_targets, thermo_preds)\n",
        "        history['thermo_mae'].append(thermo_mae)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "        print(f'EC Level 1 Acc: {ec_level1_acc:.4f}, EC Level 4 Acc: {ec_level4_acc:.4f}, Thermo MAE: {thermo_mae:.4f}')\n",
        "\n",
        "        # Save checkpoint if validation loss improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, f'enzyme_gnn_epoch_{epoch+1}.pt')\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'ec_level1_acc': ec_level1_acc,\n",
        "                'ec_level4_acc': ec_level4_acc,\n",
        "                'thermo_mae': thermo_mae\n",
        "            }, checkpoint_path)\n",
        "            print(f'Checkpoint saved to {checkpoint_path}')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def plot_training_history(history, save_dir='/content/drive/MyDrive/EnzymeGNN/visualizations'):\n",
        "    \"\"\"\n",
        "    Plot training history.\n",
        "    \"\"\"\n",
        "    # Create save directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(save_dir, 'loss_curve.png'), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot EC level 1 and 4 accuracy\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['ec_level1_acc'], label='EC Level 1 Accuracy')\n",
        "    plt.plot(history['ec_level4_acc'], label='EC Level 4 Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('EC Number Prediction Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(save_dir, 'ec_accuracy.png'), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot thermostability MAE\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['thermo_mae'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MAE (°C)')\n",
        "    plt.title('Thermostability Prediction MAE')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(save_dir, 'thermo_mae.png'), dpi=300)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "2AAyqsd1TSS-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Upload your kaggle.json file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "w9iv3hrhXRBJ",
        "outputId": "cebd2878-7387-4225-dbdc-4063d2bcf57f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ccbcc5dc-6b66-4e62-9eec-60778a033dc8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ccbcc5dc-6b66-4e62-9eec-60778a033dc8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle (1).json to kaggle (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle (1).json': b'{\"username\":\"btissam12\",\"key\":\"3774017372de4e6b1b0f11569844a11b\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRRcmE79bcsI",
        "outputId": "bc5eb5e6-110a-42ca-e21c-ad6ff84c9e74"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDCI5_R0bkzz",
        "outputId": "13d1d6c8-4d9c-474f-e8de-605115042fdf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.1.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d roberthatch/nesp-kvigly-test-mutation-pdbs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw1r9f3qb39t",
        "outputId": "2bd30f1e-4903-4d14-ea67-b86fb61eb8e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1734, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nesp-kvigly-test-mutation-pdbs.zip -d /content/drive/MyDrive/EnzymeGNN/data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URTIak_hcAtq",
        "outputId": "fc8483c7-60ea-41ac-dde7-0df388e3fbca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open nesp-kvigly-test-mutation-pdbs.zip, nesp-kvigly-test-mutation-pdbs.zip.zip or nesp-kvigly-test-mutation-pdbs.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively, you can use a sample dataset for testing\n",
        "# Create a sample dataset with a few PDB files\n",
        "import os\n",
        "import urllib.request\n",
        "from Bio.PDB import PDBParser, PDBIO\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('/content/drive/MyDrive/EnzymeGNN/data/sample', exist_ok=True)\n",
        "\n",
        "# Download a few sample PDB files\n",
        "sample_pdbs = [\n",
        "    ('1a0j', 'https://files.rcsb.org/download/1A0J.pdb') ,  # Alcohol dehydrogenase (EC 1.1.1.1)\n",
        "    ('1a27', 'https://files.rcsb.org/download/1A27.pdb') ,  # Serine protease (EC 3.4.21.-)\n",
        "    ('1a3w', 'https://files.rcsb.org/download/1A3W.pdb')    # Transferase (EC 2.7.1.-)\n",
        "]\n",
        "\n",
        "# Download and save PDB files\n",
        "for pdb_id, url in sample_pdbs:\n",
        "    pdb_file = f'/content/drive/MyDrive/EnzymeGNN/data/sample/{pdb_id}.pdb'\n",
        "    if not os.path.exists(pdb_file):\n",
        "        print(f'Downloading {pdb_id}...')\n",
        "        urllib.request.urlretrieve(url, pdb_file)\n",
        "        print(f'Saved to {pdb_file}')\n",
        "\n",
        "# Create a sample metadata CSV file\n",
        "import pandas as pd\n",
        "\n",
        "# Sample metadata\n",
        "metadata = {\n",
        "    'protein_id': ['1a0j', '1a27', '1a3w'],\n",
        "    'ec_numbers': ['1.1.1.1', '3.4.21.0', '2.7.1.0'],\n",
        "    'thermostability': [45.2, 52.8, 38.6],\n",
        "    'sequence': [\n",
        "        'MKGFAMLSIGKVGWIEKEKPAPGPFDAIVRPLAVAPCTSDIHTVFEGAIGERHNMILGHEAVGEVVEVGSEVKDFKPGDRVIVPCTTPDWRSLEVQAGFQQHSNGMLAGWKFSNFKDGVFGEYFHVNDADMNLAILPKDMPLENAVMITDMMTTGFHGAELADIQMGSSVVVIGIGAVGLMGIAGAKLRGAGRIIGVGSRPICVEAAKFYGATDILNYKNGHIVDQVMKLTNGKGVDRVIMAGGGSETLSQAVSMVKPGGIISNINYHGSGDALLIPRVEWGCGMAHKTIKGGLCPGGRLRAEMLRDMVVYNRVDLSKLVTHVYHGFDHIEEALLLMKDKPKDLIKAVVIL',\n",
        "        'IVGGYTCGANTVPYQVSLNSGYHFCGGSLINSQWVVSAAHCYKSGIQVRLGEDNINVVEGNEQFISASKSIVHPSYNSNTLNNDIMLIKLKSAASLNSRVASISLPTSCASAGTQCLISGWGNTKSSGTSYPDVLKCLKAPILSDSSCKSAYPGQITSNMFCAGYLEGGKDSCQGDSGGPVVCSGKLQGIVSWGSGCAQKNKPGVYTKVCNYVSWIKQTIASN',\n",
        "        'MKLKGLDVVVGYSTDYLAGCNHLPWTEKLKTILRDIGFHSSRWVTQVDDGIDGLAQYIFENQLSEGLDSLKLVSVIHKDVEIVSQETVGQTLPWPKIDKFSDTPFYQRWMNFYISDEDNYLIGSNVYIGTDVGNNELTIIHTDNQERTPVVYLKGDLVWEGGNLDSLEGKQVIEHSYLDGAFYRLNSPWCNDTLFSRQRYKVNLKFPGGEHVYIAKQFVEGGGLDVVKFQNPNENFVGAAVLAKGDWQKIIDNPNVVLVDTVSGFGKDYYKVNPNELRVWDYTDVNKTPVVYLKGDLVWDGGNLDTLEGKQVIEHSYLDGAFYRLNSPWCNDTLFSRQRYKVNLKFPGGEHVYIAKQFVEGGGLDVVKFQNPNENFVGAAVLAKGDWQKIIDNPNVVLVDTVSGFGKDYYKV'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "metadata_df = pd.DataFrame(metadata)\n",
        "metadata_df.to_csv('/content/drive/MyDrive/EnzymeGNN/data/sample_metadata.csv', index=False)\n",
        "print('Sample metadata saved to /content/drive/MyDrive/EnzymeGNN/data/sample_metadata.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSrzt9AJcN1m",
        "outputId": "cd1e6454-5008-4244-e57c-a8e519f0532f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample metadata saved to /content/drive/MyDrive/EnzymeGNN/data/sample_metadata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = create_dataloader(\n",
        "        pdb_dir='/content/drive/MyDrive/EnzymeGNN/data/sample',\n",
        "        csv_file='/content/drive/MyDrive/EnzymeGNN/data/sample_metadata.csv',\n",
        "        batch_size=2,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = create_dataloader(\n",
        "        pdb_dir='/content/drive/MyDrive/EnzymeGNN/data/sample',\n",
        "        csv_file='/content/drive/MyDrive/EnzymeGNN/data/sample_metadata.csv',\n",
        "        batch_size=2,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Get a sample batch to check dimensions\n",
        "    for batch in train_loader:\n",
        "        print(\"Atom features shape:\", batch['atom_features'].shape)\n",
        "        print(\"Residue features shape:\", batch['residue_features'].shape)\n",
        "\n",
        "        # Check if sequence_features is a list or tensor\n",
        "        if isinstance(batch['sequence_features'], list):\n",
        "            print(\"Sequence features is a list with length:\", len(batch['sequence_features']))\n",
        "            # Convert list to tensor if needed\n",
        "            if len(batch['sequence_features']) > 0 and isinstance(batch['sequence_features'][0], torch.Tensor):\n",
        "                # Stack tensors in the list\n",
        "                sequence_tensor = torch.stack(batch['sequence_features'])\n",
        "                print(\"Converted sequence features shape:\", sequence_tensor.shape)\n",
        "                # Update the batch with the tensor\n",
        "                batch['sequence_features'] = sequence_tensor\n",
        "            else:\n",
        "                print(\"Sequence features list contains non-tensor elements\")\n",
        "        else:\n",
        "            print(\"Sequence features shape:\", batch['sequence_features'].shape)\n",
        "        break\n",
        "\n",
        "    # Create model with correct dimensions\n",
        "    atom_feature_dim = batch['atom_features'].shape[1]  # Use actual dimension\n",
        "    residue_feature_dim = batch['residue_features'].shape[1]  # Use actual dimension\n",
        "\n",
        "    print(f\"Using atom_feature_dim={atom_feature_dim}, residue_feature_dim={residue_feature_dim}\")\n",
        "\n",
        "    # Create model with the correct dimensions\n",
        "    model = EnzymeGNN(\n",
        "        atom_feature_dim=atom_feature_dim,\n",
        "        residue_feature_dim=residue_feature_dim,\n",
        "        hidden_dim=64,\n",
        "        num_layers=3,\n",
        "        fusion_dim=128\n",
        "    )\n",
        "\n",
        "    # Create loss function\n",
        "    criterion = EnzymeGNNLoss(\n",
        "        ec_weights=[1.0, 1.0, 1.0, 1.0],\n",
        "        thermo_weight=1.0,\n",
        "        active_site_weight=0.5\n",
        "    )\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train model\n",
        "    model, history = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        num_epochs=10,  # Reduced for sample\n",
        "        device=device,\n",
        "        checkpoint_dir='/content/drive/MyDrive/EnzymeGNN/models'\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(history, save_dir='/content/drive/MyDrive/EnzymeGNN/visualizations')\n",
        "\n",
        "    print('Training completed successfully!')\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qMkJjZ3d2vz",
        "outputId": "6c678288-9a00-418b-c37c-f1277d577c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    }
  ]
}